<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
  /* Design Credits: Jon Barron, Deepak Pathak and Saurabh Gupta*/
  a {
  color: #1772d0;
  text-decoration:none;
  }
  a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
  }
  body,td,th {
     font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
     font-size: 15px;
     font-weight: 400
  }
  heading {
     font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
     font-size: 19px;
     font-weight: 1000
  }
  strong {
     font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
     font-size: 15px;
     font-weight: 800
  }
  strongred {
     font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
     color: 'red' ;
     font-size: 15px;
     font-weight: 800
  }
  sectionheading {
     font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
     font-size: 22px;
     font-weight: 600
  }
  pageheading {
     font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
     font-size: 38px;
     font-weight: 400
  }
  </style>
  <!-- <link rel="icon" type="image/png" href="images/W.png"> -->
  <script type="text/javascript" src="js/hidebib.js"></script>
  <title>Aditya Kusupati</title>
  <meta name="Aditya Kusupati's Washington Homepage" http-equiv="Content-Type" content="Aditya Kusupati's Washington Homepage">
  <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

  <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-JF89K2ZW50"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-JF89K2ZW50');
</script>

  <!-- Start : Google Analytics Code -->
  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-90394621-1', 'auto');
  ga('send', 'pageview');

  </script>
  <!-- End : Google Analytics Code -->
  <!-- Scramble Script by Jeff Donahue -->
  <script src="js/scramble.js"></script>
</head>

<body>
<table width="840" border="0" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr><td>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <p align="center">
     <pageheading>Aditya Kusupati</pageheading><br>
     <b>email</b>:
     <font id="email" style="display:inline;">
        <noscript><i>Please enable Javascript to view</i></noscript>
     </font>
     <script>
     emailScramble = new scrambledString(document.getElementById('email'),
          'emailScramble', 'kusupati@google.cmo / kusupati@cs.washington.eud',
          [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,19,18,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,48,47]);
     </script>
  </p>

  <tr>
     <td width="32%" valign="top"><a href="#Bio"><img src="images/AdityaKusupati.jpg" width="100%" style="border-radius:15px"></a>
     <p align=center>
     <a href="docs/AdityaKusupati_Research_CV.pdf" target="_blank">CV</a> | <a href="https://scholar.google.co.in/citations?user=qULx8g8AAAAJ&hl=en" target="_blank">Scholar</a> | <a href="https://twitter.com/adityakusupati" target="_blank">Twitter</a> 
       <br> <a href="docs/AdityaKusupati_Research_Statement.pdf" target="_blank">Research Statement</a> 
<!--        | <a href="docs/AdityaKusupati_Teaching_Statement.pdf" target="_blank">Teaching Statement</a> | <a href="docs/AdityaKusupati_Diversity_Statement.pdf" target="_blank">Diversity Statement</a> -->
     </p> 
     </td>
     <td width="68%" valign="top" align="justify" id="Bio">

     <p>I am a Senior Research Scientist at Google DeepMind. I got my PhD from the University of Washington jointly advised by <a href="https://homes.cs.washington.edu/~ali/" target="_blank">Ali Farhadi</a> and <a href="https://sham.seas.harvard.edu/" target="_blank">Sham Kakade</a>. I also worked closely with <a href="http://www.prateekjain.org/" target="_blank">Prateek Jain</a> and <a href="https://research.google/people/RahulSukthankar/" target="_blank">Rahul Sukthankar</a> at Google Research &rarr; DeepMind as a <a href="https://research.google/people/AdityaKusupati/" target="_blank">Student Researcher</a>. My research interests lie in the intersection of Machine Learning & Search with a focus on real-world deployability -- <font size="-1"><i>some of the algorithms (might) have serendipitously found their way into Google, OpenAI, Apple, Pinterest and Microsoft products with massive impact</i></font>.</p> 

     <p>Before joining PhD, I spent two amazing years as a Research Fellow at Microsoft Research India with <a href="http://manikvarma.org/" target="_blank">Manik Varma</a> and <a href="http://www.prateekjain.org/" target="_blank">Prateek Jain</a>. In a past life, I earned a Bachelor's in CS with Honours and a Minor in EE from IIT Bombay where I had the pleasure of working with <a href="https://www.cse.iitb.ac.in/~soumen/" target="_blank">Soumen Chakrabarti</a>.</p>

<!--      <p><b>I am on the job market for faculty and industry positions during 2023-24 cycle!</b> Feel free to reach out if you think I might be a fit.</p> -->

     <!-- <p><b>Undergraduate and Masters students at UW:</b> I am actively looking for motivated students to work with me on problems spanning Machine Learning, Vision, NLP and Search. If you are interested in working with me and rest of the RAIVN Lab please fill out <a href="https://forms.gle/iJszFok1VDmpWLy78" target="_blank">this form</a> and drop an email!</p> -->

     <!-- <p><i>Pro Bono:</i> I have set aside 1 hr every week to help people/organizations who/that might benefit from my insights in using ML and CS to solve problems with <i>societal impact</i>. PhD applicants from <i>underrepresented communities</i> can also use this time to get feedback from me on their applications. Contact me via email to set up the slot.</p> -->

     </td>
  </tr>
</table>

<!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td id="News">
     <sectionheading>&nbsp;&nbsp;News</sectionheading>
     <ul>
     <!-- <li> Checkout our new work on multi-class classification in sub-linear costs: <a href="#Kusupati21">LLC</a>!!</li> -->
     <!-- <li> <a href="http://cvpr2021.thecvf.com/node/184" target="_blank">Outstanding reviewer</a> award for CVPR '21!</li> -->
     <!-- <li> <a href="https://icml.cc/Conferences/2021/Reviewers#expertreviewers" target="_blank">Expert Reviewer</a> for ICML '21!</li> -->
     <!-- <li> Excited to organize <a href="https://rethinkingmlpapers.github.io/" target="_blank">Rethinking ML Papers</a> workshop at ICLR '21!</li> -->
     <!-- <li> <a href="https://neurips.cc/Conferences/2020/ProgramCommittee" target="_blank">Top 10% reviewer</a> award for NeurIPS '20!</li> -->
     <!-- <li> <a href="#Saha20">RNNPool</a> accepted at NeurIPS '20!! - A new pooling operator?</li>  -->
     <!-- <li> <a href="https://icml.cc/Conferences/2020/Reviewers" target="_blank">Top reviewer</a> award for ICML '20!</li> -->
     <!-- <li> Two new works on resource-efficient deep learning tackling <a href="#Kusupati20">inference FLOPs</a> and <a href="#Saha20">working RAM</a>.</li> -->
     <!-- <li> Interning in summer 2020 at NVIDIA Toronto Lab with Prof. Sanja Fidler and Prof. Antonio Torralba.</li> -->
     <!-- <li> <a href="#Kusupati20">STR</a> accepted at ICML '20 and started learnable sparsity!! - Check out the <a href="https://youtu.be/Hrki0p_gZKk">talk</a>.</li> -->
     <!-- <li> <a href="#Prabhu20">XReg</a> presented as a long oral at WSDM '20!!</li> -->
     <!-- <li> Our <a href="#Roy19">BuildSys '19 paper</a> won the <b>Best Paper Runner-Up</b> Award!! - <a href="https://news.cs.washington.edu/2019/12/23/allen-schools-aditya-kusupati-earns-best-paper-runner-up-at-buildsys-2019-for-new-low-power-deep-learning-algorithm-for-radar-classification/" target="_blank">Article</a> <font size="1.5">in Allen School News.</font> -->
     <!-- <li> <a href="#Prabhu20">Paper</a> on extreme multi-label regression accepted at WSDM '20 as a Long Oral presentation!</li> -->
     <!-- <li> <a href="https://neurips.cc/Conferences/2019/Reviewers" target="_blank">Best reviewer</a> (top 400) award for NeurIPS '19!</li> -->
     <!-- <li> Attending <a href="https://heidelberg-laureate-forum.org" target="_blank">Heidelberg Laureate Forum</a> (HLF '19) as a Young Researcher.</li> -->
     <!-- <li> <a href="#EdgeML">EdgeML</a> revamped for the next (v3) release. TF 1.x and PyTorch support including CUDA optimized FastGRNN!</li> -->
     <!-- <li> <a href="#Roy19">Oral and demo</a> accepted at BuildSys '19 on RNNs for on-device radar based detection/classification.</li> -->
     <!-- <li> Accepted CS PhD offer at University of Washington. Time to Se(a)ttle <font size="1">(Settle in Seattle. I am sorry.)</font>!</li> -->
     <!-- <li> <a href="#Kusupati18">NeurIPS '18 paper</a> on resource efficient deep learning (RNNs) got covered in <a href="https://www.microsoft.com/en-us/research/blog/fast-accurate-stable-and-tiny-breathing-life-into-iot-devices-with-an-innovative-algorithmic-approach/" target="_blank">Microsoft Research Blog</a>.</li> -->
     <!-- </ul>
  </td></tr>
</table> -->

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td id="Preprints"> <sectionheading>&nbsp;&nbsp;Preprints</sectionheading><div style="float: right;">*<font size="1"> - equal contribution</font></div></td></tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="6.5">
  <tr>
   <td width="0%" valign="top" align="center"><a href="pubs/shen24.pdf" target="_blank"><img src="images/wallingford20.jpg" alt="SupDec" width="100%" style="border-radius:15px"></a>
   <td width="100%" valign="top">
      <p><a href="pubs/shen24.pdf" target="_blank" id="Shen24">
      <heading>Superposed Decoding: Multiple Generations from a Single Autoregressive Inference Pass</heading></a><br>
      Ethan Shen, Alan Fan, Sarah M Pratt, Jae Sung Park, Matthew Wallingford, Sham M. Kakade, Ari Holtzman, Ranjay Krishna, Ali Farhadi, <strong>Aditya Kusupati</strong>.<br>
      Under Review, 2024<br>
  </p>
      <div class="paper" id="shen24">
      <a href="pubs/shen24.pdf" target="_blank">pdf</a> /
      <a href="https://arxiv.org/abs/2405.18400" target="_blank">arXiv</a> /
      <a href="https://github.com/RAIVNLab/SuperposedDecoding" target="_blank">code</a>
      <br>
      </div>
   </td>
</tr>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="6.5">
  <tr>
   <td width="0%" valign="top" align="center"><a href="pubs/devvrit23.pdf" target="_blank"><img src="images/wallingford20.jpg" alt="MatFormer" width="100%" style="border-radius:15px"></a>
   <td width="100%" valign="top">
      <p><a href="pubs/devvrit23.pdf" target="_blank" id="Devvrit23">
      <heading>MatFormer: Nested Transformer for Elastic Inference</heading></a><br>
      Devvrit*, Sneha Kudugunta*, <strong>Aditya Kusupati*</strong>, Tim Dettmers, Kaifeng Chen, Inderjit Dhillon, Yulia Tsvetkov, Hannaneh Hajishirzi, Sham Kakade, Ali Farhadi, Prateek Jain.<br>
      Under Review, 2024<br>
  </p>
      <div class="paper" id="devvrit23">
      <a href="pubs/devvrit23.pdf" target="_blank">pdf</a> /
      <a href="http://arxiv.org/abs/2310.07707" target="_blank">arXiv</a> /
      <a href="https://github.com/RAIVNLab/MatFormer-OLMo" target="_blank">code (MatLM)</a> /
      <a href="https://github.com/google-research/scenic/tree/main/scenic/projects/matvit" target="_blank">code (MatViT)</a>
      <br>
      Also presented at the ENLSP (&#x1F3C6; <b>Best Paper Award</b>) and WANT workshops @ NeurIPS, 2023<br>
      </div>
   </td>
</tr>





<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td id="ConfPublications"> <sectionheading>&nbsp;&nbsp;Conference Publications</sectionheading></td></tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="6.5">
  <tr>
   <td width="0%" valign="top" align="center"><a href="pubs/kumar23.pdf" target="_blank"><img src="images/wallingford20.jpg" alt="EHI" width="100%" style="border-radius:15px"></a>
   <td width="100%" valign="top">
      <p><a href="pubs/kumar23.pdf" target="_blank" id="Kumar23">
      <heading>EHI: End-to-end learning of Hierarchical Index for Efficient Dense Retrieval</heading></a><br>
      <a href="https://ramnathkumar181.github.io/" target="_blank">Ramnath Kumar*</a>, <a href="http://anshulmittal.org/" target="_blank">Anshul Mittal*</a>, <a href="https://nilesh2797.github.io/" target="_blank">Nilesh Gupta</a>, <strong>Aditya Kusupati</strong>, <a href="https://www.cs.utexas.edu/~inderjit/" target="_blank">Inderjit S Dhillon</a>, <a href="http://prateekjain.org" target="_blank">Prateek Jain</a><br>
      Transactions on Machine Learning Research (<b>TMLR</b>), 2024<br>
  </p>
      <div class="paper" id="kumar23">
      <a href="pubs/kumar23.pdf" target="_blank">pdf</a> /
      <a href="https://arxiv.org/abs/2310.08891" target="_blank">arXiv</a> /
      <a href="#" target="_blank">code</a>
      <br>
      </div>
   </td>
</tr> 

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="6.5">
  <tr>
   <td width="0%" valign="top" align="center"><a href="pubs/lee24.pdf" target="_blank"><img src="images/wallingford20.jpg" alt="Gecko" width="100%" style="border-radius:15px"></a>
   <td width="100%" valign="top">
      <p><strongred style="color:red;">[Google Technical Report]</strongred><a href="pubs/lee24.pdf" target="_blank" id="Salehi23">
      <heading>Gecko: Versatile Text Embeddings Distilled from Large Language Models</heading></a><br>
      Jinhyuk Lee, Zhuyun Dai, Xiaoqi Ren, Blair Chen, Daniel Cer, Jeremy R. Cole, Kai Hui, Michael Boratko, Rajvi Kapadia, Wen Ding, Yi Luan, Sai Meher Karthik Duddu, Gustavo Hernandez Abrego, Weiqiang Shi, Nithi Gupta, <strong>Aditya Kusupati</strong>, Prateek Jain, Siddhartha Reddy Jonnalagadda, Ming-Wei Chang, Iftekhar Naim<br>
      Google Technical Report, 2024<br>
  </p>
      <div class="paper" id="kudugunta23">
      <a href="pubs/lee24.pdf" target="_blank">pdf</a> /
      <a href="https://arxiv.org/abs/2403.20327" target="_blank">arXiv</a> /
      <a href="#" target="_blank">code</a>
      <br>
      </div>
   </td>
</tr>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="6.5">
  <tr>
   <td width="0%" valign="top" align="center"><a href="pubs/salehi23.pdf" target="_blank"><img src="images/wallingford20.jpg" alt="SHARCS" width="100%" style="border-radius:15px"></a>
   <td width="100%" valign="top">
      <p><strongred style="color:red;">[EMNLP'23 Findings]</strongred><a href="pubs/salehi23.pdf" target="_blank" id="Salehi23">
      <heading>SHARCS: Efficient Transformers through Routing with Dynamic Width Sub-networks</heading></a><br>
      Mohammadreza Salehi, Sachin Mehta, <strong>Aditya Kusupati</strong>, Ali Farhadi, Hanna Hajishirzi.<br>
      Empirical Methods in Natural Language Processing (<b>EMNLP</b>) Findings, 2023<br>
  </p>
      <div class="paper" id="kudugunta23">
      <a href="pubs/salehi23.pdf" target="_blank">pdf</a> /
      <a href="https://arxiv.org/abs/2310.12126" target="_blank">arXiv</a> /
      <a href="#" target="_blank">code</a>
      <br>
      </div>
   </td>
</tr>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="6.5">
  <tr>
   <td width="0%" valign="top" align="center"><a href="pubs/kudugunta23.pdf" target="_blank"><img src="images/wallingford20.jpg" alt="MADLAD" width="100%" style="border-radius:15px"></a>
   <td width="100%" valign="top">
      <p><strongred style="color:red;">[NeurIPS'23 D&B]</strongred><a href="pubs/kudugunta23.pdf" target="_blank" id="Kudugunta23">
      <heading>MADLAD-400: Monolingual And Document-Level Large Audited Dataset</heading></a><br>
      Sneha Kudugunta, Isaac Rayburn Caswell, Biao Zhang, Xavier Garcia, Christopher A. Choquette-Choo, Katherine Lee, Derrick Xin, <strong>Aditya Kusupati</strong>, Romi Stella, Ankur Bapna, Orhan Firat<br>
      Neural Information Processing Systems (<b>NeurIPS</b>) Datasets & Benchmarks Track, 2023<br>
  </p>
      <div class="paper" id="kudugunta23">
      <a href="pubs/kudugunta23.pdf" target="_blank">pdf</a> /
      <a href="https://arxiv.org/abs/2309.04662" target="_blank">arXiv</a> /
      <a href="https://github.com/google-research/google-research/tree/master/madlad_400" target="_blank">code</a>
      <br>
      </div>
   </td>
</tr>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="6.5">
  <tr>
   <td width="0%" valign="top" align="center"><a href="https://arxiv.org/pdf/2307.05663.pdf" target="_blank"><img src="images/wallingford20.jpg" alt="Objaverse-XL" width="100%" style="border-radius:15px"></a>
   <td width="100%" valign="top">
      <p><strongred style="color:red;">[NeurIPS'23 D&B]</strongred><a href="https://arxiv.org/pdf/2307.05663.pdf" target="_blank" id="Deitke23">
      <heading>Objaverse-XL: A Universe of 10M+ 3D Objects</heading></a><br>
      Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, <strong>Aditya Kusupati</strong>, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, Eli VanderBilt, Aniruddha Kembhavi, Carl Vondrick, Georgia Gkioxari, Kiana Ehsani, Ludwig Schmidt, Ali Farhadi<br>
      Neural Information Processing Systems (<b>NeurIPS</b>) Datasets & Benchmarks Track, 2023<br>
  </p>
      <div class="paper" id="deitke23">
      <a href="https://arxiv.org/pdf/2307.05663.pdf" target="_blank">pdf</a> /
      <a href="https://arxiv.org/abs/2307.05663" target="_blank">arXiv</a> /
      <a href="https://huggingface.co/datasets/allenai/objaverse-xl" target="_blank">dataset</a> /
      <a href="https://objaverse.allenai.org/" target="_blank">project page</a>
      <br>
      </div>
   </td>
</tr>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="6.5">
  <tr>
   <td width="0%" valign="top" align="center"><a href="pubs/wallingford23a.pdf" target="_blank"><img src="images/wallingford20.jpg" alt="AdANNS" width="100%" style="border-radius:15px"></a>
   <td width="100%" valign="top">
      <p><strongred style="color:red;">[NeurIPS'23]</strongred><a href="pubs/wallingford23a.pdf" target="_blank" id="Wallingford23a">
      <heading>Neural Priming for Sample-Efficient Adaptation</heading></a><br>
      <a href="https://mattwallingford.github.io/">Matthew Wallingford</a>*, <a href="https://vkramanuj.github.io/" target="_blank">Vivek Ramanujan</a>*, Alex Fang, <strong>Aditya Kusupati</strong>, <a href="https://roozbehm.info/" target="_blank"> Roozbeh Mottaghi</a>, <a href="https://anikem.github.io/" target="_blank">Aniruddha Kembhavi</a>, <a href="https://people.csail.mit.edu/ludwigs/" target="_blank">Ludwig Schmidt</a> and <a href="https://homes.cs.washington.edu/~ali/" target="_blank">Ali Farhadi</a><br>
      Neural Information Processing Systems (<b>NeurIPS</b>), 2023<br>
  </p>
      <div class="paper" id="wallingford23a">
      <!-- <a href="javascript:toggleblock('rege23abs')">abstract</a> / -->
      <!-- <a shape="rect" href="javascript:togglebib('rege23')" class="togglebib">bibtex</a> / -->
      <a href="pubs/wallingford23a.pdf" target="_blank">pdf</a> /
      <a href="https://arxiv.org/abs/2306.10191" target="_blank">arXiv</a> /
      <a href="https://github.com/RAIVNLab/neural-priming" target="_blank">code</a>
      <br>
      </div>
   </td>
</tr>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="6.5">
  <tr>
   <td width="0%" valign="top" align="center"><a href="pubs/rege23.pdf" target="_blank"><img src="images/wallingford20.jpg" alt="AdANNS" width="100%" style="border-radius:15px"></a>
   <td width="100%" valign="top">
      <p><strongred style="color:red;">[NeurIPS'23]</strongred><a href="pubs/rege23.pdf" target="_blank" id="Rege23">
      <heading>AdANNS: A Framework for Adaptive Semantic Search</heading></a><br>
      <a href="https://aniketrege.github.io/about/" target="_blank">Aniket Rege*</a>, <strong>Aditya Kusupati*</strong>, Sharan Ranjit, Alan Fan, Qingqing Cao, <a href="https://sham.seas.harvard.edu/" target="_blank">Sham Kakade</a>, <a href="http://prateekjain.org" target="_blank">Prateek Jain</a>, and <a href="https://homes.cs.washington.edu/~ali/" target="_blank">Ali Farhadi</a><br>
      Neural Information Processing Systems (<b>NeurIPS</b>), 2023<br>
  </p>
      <div class="paper" id="rege23">
      <!-- <a href="javascript:toggleblock('rege23abs')">abstract</a> / -->
      <!-- <a shape="rect" href="javascript:togglebib('rege23')" class="togglebib">bibtex</a> / -->
      <a href="pubs/rege23.pdf" target="_blank">pdf</a> /
      <a href="https://arxiv.org/abs/2305.19435" target="_blank">arXiv</a> /
      <a href="https://github.com/RAIVNLab/AdANNS" target="_blank">code</a>
      <br>
      Also presented at the PMLDC workshops @ ICLR, 2023<br>
      <p align="justify"> <i id="rege23abs">Web-scale search systems use a large neural network to embed the query which is then hooked into a separate approximate nearest neighbour search (ANNS) pipeline to retrieve similar data points. Such approaches use a rigid -- potentially high-dimensional -- representation out of encoder to perform the entire search. This can be far from optimal accuracy-compute trade-off. In this paper, we argue that in different stages of ANNS, we can use representations of different capacities, adaptive representations, to ensure that the accuracy-compute tradeoff can be met nearly optimally. In particular, we introduce AdANNS, a novel ANNS design paradigm that explicitly leverages the flexibility and adaptive capabilities of the recently introduced Matryoshka Representations (Kusupati et al., 2022). We demonstrate that using AdANNS to construct the search data structure (AdANNS-C) provides state-of-the-art accuracy-compute tradeoff; AdANNS powered inverted file index (IVF) is up to 1.5% more accurate or up to 100x faster ImageNet-1K retrieval. We also show that matryoshka representations can power compute-aware adaptive search during inference (AdANNS-D) on a fixed ANNS (IVF) structure and be up to 16x faster for similar accuracy. Finally, we explore the applicability of adaptive representations across ANNS building blocks and further analyze the choice of matryoshka representations for semantic search. Code is open-sourced at https://github.com/RAIVNLab/AdANNS.</a>.</i><p>

<pre xml:space="preserve">
@article{Rege23
  author    = {Rege, Aniket and Kusupati, Aditya 
    and Ranjit, Sharan and Fan, Alan
    and Cao, Qingqing
    and Kakade, Sham and Prateek Jain
    and Farhadi, Ali},
  title     = {Adaptive Representations for Semantic Search.},
  booktitle = {arXiv preprint arXiv:},
  year      = {2023},
}
</pre>
      </div>
   </td>
</tr>


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="6.5">
  <tr>
   <td width="0%" valign="top" align="center"><a href="pubs/wallingford20.pdf" target="_blank"><img src="images/wallingford20.jpg" alt="FLUID" width="100%" style="border-radius:15px"></a>
   <td width="100%" valign="top">
      <p><strongred style="color:red;">[TMLR'23]</strongred><a href="pubs/wallingford20.pdf" target="_blank" id="Wallingford20">
      <heading>FLUID: A Unified Evaluation Framework for Flexible Sequential Data</heading></a><br>
      <a href="https://mattwallingford.github.io/">Matthew Wallingford</a>, <strong>Aditya Kusupati</strong>, <a href="https://homes.cs.washington.edu/~keivan/" target="_blank">Keivan Alizadeh-Vahid</a>, <a href="http://aaronwalsman.com/" target="_blank">Aaron Walsman</a>, <a href="https://anikem.github.io/" target="_blank">Aniruddha Kembhavi</a> and <a href="https://homes.cs.washington.edu/~ali/" target="_blank">Ali Farhadi</a><br>
      Transactions on Machine Learning Research (<b>TMLR</b>), 2023<br>
  </p>
      <div class="paper" id="wallingford20">
      <!-- <a href="javascript:toggleblock('wallingford20abs')">abstract</a> / -->
      <!-- <a shape="rect" href="javascript:togglebib('wallingford20')" class="togglebib">bibtex</a> / -->
      <a href="pubs/wallingford20.pdf" target="_blank">pdf</a> /
      <a href="https://arxiv.org/abs/2007.02519" target="_blank">arXiv</a> /
      <a href="https://github.com/RAIVNLab/InTheWild" target="_blank">code</a> /
      <a href="https://raivn.cs.washington.edu/projects/FLUID" target="_blank">project page</a>
      <br>
      <p align="justify"> <i id="wallingford20abs">Enabling robust intelligence in the real-world entails systems that offer continuous inference while learning from varying amounts of data and supervision. The machine learning community has organically broken down this challenging goal into manageable sub-tasks such as supervised, few-shot, and continual learning. In light of substantial progress on each sub-task, we pose the question, “How well does this progress translate to more practical scenarios?” To investigate this question, we construct a new framework, FLUID, which removes certain assumptions made by current experimental setups while integrating these sub-tasks via the following design choices -- consuming sequential data, allowing for flexible training phases, being compute aware, and working in an open-world setting. Evaluating a broad set of methods on FLUID leads to new insights including strong evidence that methods are overfitting to their experimental setup. For example, we find that representative few-shot methods are substantially worse than simple baselines, self-supervised representations from MoCo fail to learn new classes when the downstream task contains a mix of new and old classes, and pretraining largely mitigates the problem of catastrophic forgetting. Finally, we propose two new simple methods which outperform all other evaluated methods which further questions our progress towards robust, real-world systems</a>.</i><p>

<pre xml:space="preserve">
@article{Wallingford20
  author    = {Wallingford, Matthew and Kusupati, Aditya 
    and Alizadeh-Vahid, Keivan and Walsman, Aaron and 
    Kembhavi, Aniruddha and Farhadi, Ali},
  title     = {FLUID: A Unified Evaluation Framework 
    for Flexible Sequential Data.},
  booktitle = {Transactions on Machine Learning Research},
  year      = {2023},
}
</pre>
      </div>
   </td>
</tr>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="6.5">
  <tr>
   <td width="0%" valign="top" align="center"><a href="pubs/wallingford23.pdf" target="_blank"><img src="images/wallingford20.jpg" alt="NRC" width="100%" style="border-radius:15px"></a>
   <td width="100%" valign="top">
      <p><strongred style="color:red;">[ICLR'23]</strongred><a href="pubs/wallingford23.pdf" target="_blank" id="Wallingford23">
      <heading>Neural Radiance Field Codebooks</heading></a><br>
      <a href="https://mattwallingford.github.io/">Matthew Wallingford</a>, <strong>Aditya Kusupati</strong>, Alex Fang, <a href="https://vkramanuj.github.io/" target="_blank">Vivek Ramanujan</a>, <a href="https://anikem.github.io/" target="_blank">Aniruddha Kembhavi</a>, <a href="https://roozbehm.info/" target="_blank"> Roozbeh Mottaghi</a> and <a href="https://homes.cs.washington.edu/~ali/" target="_blank">Ali Farhadi</a><br>
      International Conference on Learning Representations (<b>ICLR</b>), 2023<br>
  </p>
      <div class="paper" id="wallingford23">
      <!-- <a href="javascript:toggleblock('wallingford23abs')">abstract</a> / -->
      <!-- <a shape="rect" href="javascript:togglebib('wallingford23')" class="togglebib">bibtex</a> / -->
      <a href="pubs/wallingford23.pdf" target="_blank">pdf</a> /
      <a href="https://arxiv.org/abs/2301.04101" target="_blank">arXiv</a> /
      <a href="#" target="_blank">code</a> /
      <a href="#" target="_blank">project page</a>
      <br>
      <p align="justify"> <i id="wallingford23abs">Compositional representations of the world are a promising step towards enabling high-level scene understanding and efficient transfer to downstream tasks. Learning such representations for complex scenes and tasks remains an open challenge. Towards this goal, we introduce Neural Radiance Field Codebooks (NRC), a scalable method for learning object-centric representations through novel view reconstruction. NRC learns to reconstruct scenes from novel views using a dictionary of object codes which are decoded through a volumetric renderer. This enables the discovery of reoccurring visual and geometric patterns across scenes which are transferable to downstream tasks. We show that NRC representations transfer well to object navigation in THOR, outperforming 2D and 3D representation learning methods by 3.1% success rate. We demonstrate that our approach is able to perform unsupervised segmentation for more complex synthetic (THOR) and real scenes (NYU Depth) better than prior methods (29% relative improvement). Finally, we show that NRC improves on the task of depth ordering by 5.5% accuracy in THOR.</a>.</i><p>

<pre xml:space="preserve">
@article{Wallingford23
  author    = {Wallingford, Matthew and Kusupati, Aditya 
    and Fang, Alex and Ramanujan, Vivek and Kembhavi, Aniruddha
    and Mottaghi, Roozbeh and Farhadi, Ali},
  title     = {Neural Radiance Field Codebooks.},
  booktitle = {International Conference on Learning Representations},
  year      = {2023},
}
</pre>
      </div>
   </td>
</tr>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="6.5">
  <tr>
   <td width="0%" valign="top" align="center"><a href="pubs/kusupati22.pdf" target="_blank"><img src="images/kusupati22.png" alt="MRL" width="100%" style="border-radius:15px"></a>
   <td width="100%" valign="top">
      <p><strongred style="color:red;">[NeurIPS'22]</strongred><a href="pubs/kusupati22.pdf" target="_blank" id="Kusupati22">
      <heading>Matryoshka Representation Learning</heading></a><br>
      <strong>Aditya Kusupati*</strong>, <a href="https://sites.google.com/view/gbhatt/" target="_blank">Gantavya Bhatt*</a>, <a href="https://aniketrege.github.io/about/" target="_blank">Aniket Rege*</a>, <a href="https://mattwallingford.github.io/" target="_blank">Matthew Wallingford</a>, Aditya Sinha, <a href="https://vkramanuj.github.io/" target="_blank">Vivek Ramanujan</a>, William Howard-Snyder, Kaifeng Chen, <a href="https://sham.seas.harvard.edu/" target="_blank">Sham Kakade</a>, <a href="http://prateekjain.org" target="_blank">Prateek Jain</a>, and <a href="https://homes.cs.washington.edu/~ali/" target="_blank">Ali Farhadi</a><br>
      Neural Information Processing Systems (<b>NeurIPS</b>), 2022<br>
  </p>
      <div class="paper" id="kusupati22">
      <!-- <a href="javascript:toggleblock('kusupati22abs')">abstract</a> /
      <a shape="rect" href="javascript:togglebib('kusupati22')" class="togglebib">bibtex</a> / -->
      <a href="pubs/kusupati22.pdf" target="_blank">pdf</a> /
      <a href="docs/reviews/kusupati22-neurips-reviews.pdf" target="_blank">reviews</a> /
      <a href="https://arxiv.org/abs/2205.13147" target="_blank">arXiv</a> /
      <a href="https://github.com/RAIVNLab/MRL" target="_blank">code</a> /
      <a href="https://www.youtube.com/watch?v=g7PhVKQ81oA" target="_blank">video</a> /
      <a href="docs/posters/kusupati22Poster.pdf" target="_blank">poster</a>
      <br>
      Also presented at the VTTA and SSL workshops @ NeurIPS, 2022<br>
      Used by <b>Google, OpenAI, Pinterest, Nomic, HF Sentence Transformers .....</b><br>
      <p align="justify"> <i id="kusupati22abs">Learned representations are a central component in modern ML systems, serving a multitude of downstream tasks. When training such representations, it is often the case that computational and statistical constraints for each downstream task are unknown. In this context rigid, fixed capacity representations can be either over or under-accommodating to the task at hand. This leads us to ask: can we design a flexible representation that can adapt to multiple downstream tasks with varying computational resources? Our main contribution is Matryoshka Representation Learning (MRL) which encodes information at different granularities and allows a single embedding to adapt to the computational constraints of downstream tasks. MRL minimally modifies existing representation learning pipelines and imposes no additional cost during inference and deployment. MRL learns coarse-to-fine representations that are at least as accurate and rich as independently trained low-dimensional representations. The flexibility within the learned Matryoshka Representations offer: (a) up to 14x smaller embedding size for ImageNet-1K classification at the same level of accuracy; (b) up to 14x real-world speed-ups for large-scale retrieval on ImageNet-1K and 4K; and (c) up to 2% accuracy improvements for long-tail few-shot classification, all while being as robust as the original representations. Finally, we show that MRL extends seamlessly to web-scale datasets (ImageNet, JFT) across various modalities -- vision (ViT, ResNet), vision + language (ALIGN) and language (BERT).</a>.</i><p>

<pre xml:space="preserve">
@InProceedings{Kusupati21
  author    = {Kusupati, Aditya and Bhatt, Gantavya
    and Rege, Aniket and Wallingford, Matthew 
    and Sinha, Aditya and Ramanujan, Vivek 
    and Howard-Snyder, William and Chen, Kaifeng
    and Kakade, Sham and Prateek Jain
    and Farhadi, Ali},
  title     = {Matryoshka Representation Learning.},
    booktitle = {Advances in 
      Neural Information Processing Systems},
    month     = {December},
    year      = {2022},
  }
</pre>
      </div>
   </td>
</tr>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="6.5">
  <tr>
   <td width="0%" valign="top" align="center"><a href="pubs/zellers22.pdf" target="_blank"><img src="images/zellers22.png" alt="RESERVE" width="100%" style="border-radius:15px"></a>
   <td width="100%" valign="top">
      <p><strongred style="color:red;">[CVPR'22]</strongred><a href="pubs/zellers22.pdf" target="_blank" id="Zellers22">
      <heading>MERLOT RESERVE: Neural Script Knowledge through Vision and Language and Sound</heading></a><br>
      <a href="https://rowanzellers.com/">Rowan Zellers</a>, Jiasen Lu, Ximing Lu, Youngjae Yu, Yanpeng Zhao, Mohammadreza Salehi, <strong>Aditya Kusupati</strong>, Jack Hessel, <a href="https://homes.cs.washington.edu/~ali/" target="_blank">Ali Farhadi</a> and <a href="https://homes.cs.washington.edu/~yejin/">Yejin Choi</a>.<br>
      Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2022<br>
  </p>
  Oral presentation<br>
      <div class="paper" id="zellers22">
      <!-- <a href="javascript:toggleblock('zellers22abs')">abstract</a> /
      <a shape="rect" href="javascript:togglebib('zellers22')" class="togglebib">bibtex</a> / -->
      <a href="pubs/zellers22.pdf" target="_blank">pdf</a> /
      <a href="docs/reviews/zellers22-cvpr-reviews.pdf" target="_blank">reviews</a> /
      <a href="https://arxiv.org/abs/2201.02639" target="_blank">arXiv</a> /
      <a href="http://github.com/rowanz/reserve" target="_blank">code</a> /
      <a href="https://rowanzellers.com/merlotreserve" target="_blank">project page</a>
      <br>
      <p align="justify"> <i id="zellers22abs">As humans, we navigate the world through all our senses, using perceptual input from each one to correct the others. We introduce MERLOT Reserve, a model that represents videos jointly over time -- through a new training objective that learns from audio, subtitles, and video frames. Given a video, we replace snippets of text and audio with a MASK token; the model learns by choosing the correct masked-out snippet. Our objective learns faster than alternatives, and performs well at scale: we pretrain on 20 million YouTube videos.
        Empirical results show that MERLOT Reserve learns strong representations about videos through all constituent modalities. When finetuned, it sets a new state-of-the-art on both VCR and TVQA, outperforming prior work by 5% and 7% respectively. Ablations show that both tasks benefit from audio pretraining -- even VCR, a QA task centered around images (without sound). Moreover, our objective enables out-of-the-box prediction, revealing strong multimodal commonsense understanding. In a fully zero-shot setting, our model obtains competitive results on four video understanding tasks, even outperforming supervised approaches on the recently proposed Situated Reasoning (STAR) benchmark.
        We analyze why incorporating audio leads to better vision-language representations, suggesting significant opportunities for future research. We conclude by discussing ethical and societal implications of multimodal pretraining.</a>.</i><p>

<pre xml:space="preserve">
  @inproceedings{Zellers22,
    title={Merlot reserve: Neural script knowledge through vision and language and sound},
    author={Zellers, Rowan and Lu, Jiasen and Lu, Ximing and Yu, Youngjae and Zhao, Yanpeng and Salehi, Mohammadreza and Kusupati, Aditya and Hessel, Jack and Farhadi, Ali and Choi, Yejin},
    booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
    pages={16375--16387},
    year={2022}
  }
</pre>
      </div>
   </td>
</tr>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="6.5">
  <tr>
   <td width="0%" valign="top" align="center"><a href="pubs/jain22.pdf" target="_blank"><img src="images/jain22.png" alt="ProtoSound" width="100%" height="125px" style="border-radius:10px"></a>
   <td width="100%" valign="top">
      <p><strongred style="color:red;">[CHI'22]</strongred><a href="pubs/jain22.pdf" target="_blank" id="Jain22">
      <heading>ProtoSound: Personalized, Scalable Sound Recognition for d/Deaf and Hard of Hearing Users through In-the-Wild Few-Shot Interactions</heading></a><br>
      <a href="https://homes.cs.washington.edu/~djain/" target="_blank">Dhruv Jain</a>, Khoa Nguyen, Steven Goodman, Rachel Grossman-Kahn, Hung Ngo, <strong>Aditya Kusupati</strong>, <a href="https://duruofei.com/" target="_blank">Ruofei Du</a>, <a href="https://www.olwal.com/" target="_blank">Alex Olwal</a>, <a href="https://faculty.washington.edu/leahkf/" target="_blank">Leah Findlater</a> and <a href="https://jonfroehlich.github.io/" target="_blanks">Jon Froehlich</a><br>
      Conference on Human Factors in Computing Systems (<b>CHI</b>), 2022<br>
  </p>
  Oral presentation<br>
      <div class="paper" id="jain22">
      <!-- <a href="javascript:toggleblock('jain22abs')">abstract</a> /
      <a shape="rect" href="javascript:togglebib('jain22')" class="togglebib">bibtex</a> / -->
      <a href="pubs/jain22.pdf" target="_blank">pdf</a> /
      <a href="docs/reviews/jain22-chi-reviews.pdf" target="_blank">reviews</a> /
      <a href="https://arxiv.org/abs/2202.11134" target="_blank">arXiv</a> /
      <a href="https://github.com/makeabilitylab/ProtoSound" target="_blank">code</a> / 
      <a href="https://www.youtube.com/watch?v=H4JWZgAF5dY" target="_blank">video</a>
      <br>
      <p align="justify"> <i id="jain22abs"></a>Recent advances have enabled automatic sound recognition systems for deaf and hard of hearing (DHH) users on mobile devices. However, these tools use pre-trained, generic sound recognition models, which do not meet the diverse needs of DHH users. We introduce ProtoSound, an interactive system for customizing sound recognition models by recording a few examples, thereby enabling personalized and fine-grained categories. ProtoSound is motivated by prior work examining sound awareness needs of DHH people and by a survey we conducted with 472 DHH participants. To evaluate ProtoSound, we characterized performance on two real-world sound datasets, showing significant improvement over state-of-the-art (e.g., +9.7% accuracy on the first dataset). We then deployed ProtoSound's end-user training and real-time recognition through a mobile application and recruited 19 hearing participants who listened to the real-world sounds and rated the accuracy across 56 locations (e.g., homes, restaurants, parks). Results show that ProtoSound personalized the model on-device in real-time and accurately learned sounds across diverse acoustic contexts. We close by discussing open challenges in personalizable sound recognition, including the need for better recording interfaces and algorithmic improvements.</i><p>

<pre xml:space="preserve">
  @inproceedings{Jain22,
    title={ProtoSound: A Personalized and Scalable Sound Recognition System for Deaf and Hard-of-Hearing Users},
    author={Jain, Dhruv and Huynh Anh Nguyen, Khoa and M. Goodman, Steven and Grossman-Kahn, Rachel and Ngo, Hung and Kusupati, Aditya and Du, Ruofei and Olwal, Alex and Findlater, Leah and E. Froehlich, Jon},
    booktitle={CHI Conference on Human Factors in Computing Systems},
    pages={1--16},
    year={2022}
  }
</pre>
      </div>
   </td>
</tr>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="6.5">
  <tr>
   <td width="0%" valign="top" align="center"><a href="pubs/kusupati21.pdf" target="_blank"><img src="images/kusupati21.png" alt="LLC" width="100%" height="125px" style="border-radius:10px"></a>
   <td width="100%" valign="top">
      <p><strongred style="color:red;">[NeurIPS'21]</strongred><a href="pubs/kusupati21.pdf" target="_blank" id="Kusupati21">
      <heading>LLC: Accurate, Multi-purpose Learnt Low-dimensional Binary Codes</heading></a><br>
      <strong>Aditya Kusupati</strong>, <a href="https://mattwallingford.github.io/">Matthew Wallingford</a>, <a href="https://vkramanuj.github.io/" target="_blank">Vivek Ramanujan</a>, <a href="https://raghavsomani.github.io/" target="_blank">Raghav Somani</a>, <a href="https://homes.cs.washington.edu/~jspark96/" target="_blank">Jae Sung Park</a>, <a href="https://krishnap25.github.io/" target="_blank">Krishna Pillutla</a>, <a href="http://prateekjain.org" target="_blank">Prateek Jain</a>, <a href="https://sham.seas.harvard.edu/" target="_blank">Sham Kakade</a> and <a href="https://homes.cs.washington.edu/~ali/" target="_blank">Ali Farhadi</a><br>
      Neural Information Processing Systems (<b>NeurIPS</b>), 2021<br>
  </p>
      <div class="paper" id="kusupati21">
      <!-- <a href="javascript:toggleblock('kusupati21abs')">abstract</a> /
      <a shape="rect" href="javascript:togglebib('kusupati21')" class="togglebib">bibtex</a> / -->
      <a href="pubs/kusupati21.pdf" target="_blank">pdf</a> /
      <a href="docs/reviews/kusupati21-neurips-reviews.pdf" target="_blank">reviews</a> /
      <a href="https://arxiv.org/abs/2106.01487" target="_blank">arXiv</a> /
      <a href="https://github.com/RAIVNLab/LLC" target="_blank">code</a> /
      <a href="https://youtu.be/uF-2bTNLefs" target="_blank">video</a> /
      <a href="docs/posters/kusupati21Poster.pdf" target="_blank">poster</a>
      <br>
      <p align="justify"> <i id="kusupati21abs">Learning binary representations of instances and classes is a classical problem with several high potential applications. In modern settings, the compression of high-dimensional neural representations to low-dimensional binary codes is a challenging task and often require large bit-codes to be accurate. In this work, we propose a novel method for Learning Low-dimensional binary Codes (LLC) for instances as well as classes. Our method does not require any side-information, like annotated attributes or label meta-data, and learns extremely low-dimensional binary codes (~20 bits for ImageNet-1K). The learnt codes are super-efficient while still ensuring nearly optimal classification accuracy for ResNet50 on ImageNet-1K. We demonstrate that the learnt codes capture intrinsically important features in the data, by discovering an intuitive taxonomy over classes. We further quantitatively measure the quality of our codes by applying it to the efficient image retrieval as well as out-of-distribution (OOD) detection problems. For ImageNet-100 retrieval problem, our learnt binary codes outperform 16 bit HashNet using only 10 bits and also are as accurate as 10 dimensional real representations. Finally, our learnt binary codes can perform OOD detection, out-of-the-box, as accurately as a baseline that needs ~3000 samples to tune its threshold, while we require none. Code and pre-trained models are available at https://github.com/RAIVNLab/LLC</a>.</i><p>

<pre xml:space="preserve">
@InProceedings{Kusupati21
  author    = {Kusupati, Aditya and Wallingford, Matthew 
    and Ramanujan, Vivek and Somani, Raghav and 
    Park, Jae Sung and Pillutla, Krishna and
    Jain, Prateek and Kakade, Sham and Farhadi, Ali},
  title     = {LLC: Accurate, Multi-purpose 
    Learnt Low-dimensional Binary Codes},
    booktitle = {Advances in 
      Neural Information Processing Systems},
    month     = {December},
    year      = {2021},
  }
</pre>
      </div>
   </td>
</tr>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="6.5">
  <tr>
   <td width="0%" valign="top" align="center"><a href="pubs/saha20.pdf" target="_blank"><img src="images/saha20.png" alt="RNNPool" width="100%" style="border-radius:15px"></a>
   <td width="100%" valign="top">
      <p><strongred style="color:red;">[NeurIPS'20]</strongred><a href="pubs/saha20.pdf" target="_blank" id="Saha20">
      <heading>RNNPool: Efficient Non-linear Pooling for RAM Constrained Inference</heading></a><br>
      <a href="http://oindrilasaha.github.io/" target="_blank">Oindrila Saha</a>, <strong>Aditya Kusupati</strong>, <a href="http://harsha-simhadri.org/" target="_blank">Harsha Vardhan Simhadri</a>, <a href="http://manikvarma.org" target="_blank">Manik Varma</a> and <a href="http://prateekjain.org" target="_blank">Prateek Jain</a><br>
      Neural Information Processing Systems (<b>NeurIPS</b>), 2020
  </p>
    Virtual <a href="https://neurips.cc/virtual/2020/public/poster_ebd9629fc3ae5e9f6611e2ee05a31cef.html" target="_blank">Spotlight</a> presentation<br>
      <div class="paper" id="saha20">
      <!-- <a href="javascript:toggleblock('saha20abs')">abstract</a> /
      <a shape="rect" href="javascript:togglebib('saha20')" class="togglebib">bibtex</a> / -->
      <a href="pubs/saha20.pdf" target="_blank">pdf</a> /
      <a href="links/saha20reviews.html" target="_blank">reviews</a> /
      <a href="https://arxiv.org/abs/2002.11921" target="_blank">arXiv</a> /
      <a href="https://github.com/microsoft/EdgeML" target="_blank">code</a> /
      <a href="https://youtu.be/C36iQSGr_DM" target="_blank">video</a> /
      <a href="docs/posters/saha20Poster.pdf" target="_blank">poster</a> /
      <a href="https://www.microsoft.com/en-us/research/blog/seeing-on-tiny-battery-powered-microcontrollers-with-rnnpool/" target="_blank">blog 1</a>, <a href="https://towardsdatascience.com/enabling-accurate-computer-vision-on-tiny-microcontrollers-with-rnnpool-operator-and-seedot-d6944930dcf9">2</a>
      <br>
      Also <a href="pubs/saha20-wicv.pdf" target="_blank">presented</a> at the WiCV workshop @ CVPR, 2020<br>
      <p align="justify"> <i id="saha20abs">Standard Convolutional Neural Networks (CNNs) designed for computer vision tasks tend to have large intermediate activation maps. These require large working memory and are thus unsuitable for deployment on resource-constrained devices typically used for inference on the edge. Aggressively downsampling the images via pooling or strided convolutions can address the problem but leads to a significant decrease in accuracy due to gross aggregation of the feature map by standard pooling operators. In this paper, we introduce RNNPool, a novel pooling operator based on Recurrent Neural Networks (RNNs), that efficiently aggregates features over large patches of an image and rapidly downsamples activation maps.  Empirical evaluation indicates that an RNNPool layer can effectively replace multiple blocks in a variety of architectures such as MobileNets, DenseNet when applied to standard vision tasks like image classification and face detection. That is, RNNPool can significantly decrease computational complexity and peak memory usage for inference while retaining comparable accuracy. We use RNNPool with the standard S3FD architecture to construct a face detection method that achieves state-of-the-art MAP for tiny ARM Cortex-M4 class microcontrollers with under 256 KB of RAM. Code is released at <a href="https://github.com/Microsoft/EdgeML">https://github.com/Microsoft/EdgeML</a>.</i><p>

<pre xml:space="preserve">
@InProceedings{Saha20
  author    = {Saha, Oindrila and Kusupati, Aditya and 
    Simhadri, Harsha Vardhan and Varma, Manik and 
    Jain, Prateek},
  title     = {RNNPool: Efficient Non-linear Pooling 
    for RAM Constrained Inference},
  booktitle = {Advances in 
    Neural Information Processing Systems},
  month     = {December},
  year      = {2020},
}
</pre>
      </div>
   </td>
</tr>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="6.5">
  <tr>
   <td width="0%" valign="top" align="center"><a href="pubs/kusupati20.pdf" target="_blank"><img src="images/kusupati20.png" alt="STR" width="100%" style="border-radius:15px"></a>
   <td width="100%" valign="top">
      <p><strongred style="color:red;">[ICML'20]</strongred><a href="pubs/kusupati20.pdf" target="_blank" id="Kusupati20">
      <heading>Soft Threshold Weight Reparameterization for Learnable Sparsity</heading></a><br>
      <strong>Aditya Kusupati</strong>, <a href="https://vkramanuj.github.io/" target="_blank">Vivek Ramanujan*</a>, <a href="https://raghavsomani.github.io/" target="_blank">Raghav Somani*</a>, <a href="https://mitchellnw.github.io/" target="_blank">Mitchell Wortsman*</a>, <a href="http://prateekjain.org" target="_blank">Prateek Jain</a>, <a href="https://sham.seas.harvard.edu/" target="_blank">Sham Kakade</a> and <a href="https://homes.cs.washington.edu/~ali/" target="_blank">Ali Farhadi</a><br>
      International Conference on Machine Learning (<b>ICML</b>), 2020
  </p>
  Virtual <a href="https://icml.cc/virtual/2020/poster/5772">Talk</a><br>
      <div class="paper" id="kusupati20">
      <!-- <a href="javascript:toggleblock('kusupati20abs')">abstract</a> /
      <a shape="rect" href="javascript:togglebib('kusupati20')" class="togglebib">bibtex</a> / -->
      <a href="pubs/kusupati20.pdf" target="_blank">pdf</a> /
      <a href="docs/reviews/kusupati20-icml-reviews.pdf" target="_blank">reviews</a> /
      <a href="https://arxiv.org/abs/2002.03231" target="_blank">arXiv</a> /
      <a href="https://github.com/RAIVNLab/STR" target="_blank">code</a> /
      <a href="https://youtu.be/Hrki0p_gZKk" target="_blank">video</a>
      <br>

      <p align="justify"> <i id="kusupati20abs">Sparsity in Deep Neural Networks (DNNs) is studied extensively with the focus of maximizing prediction accuracy given an overall parameter budget. Existing methods rely on uniform or heuristic non-uniform sparsity budgets which have sub-optimal layer-wise parameter allocation resulting in a) lower prediction accuracy or b) higher inference cost (FLOPs). This work proposes Soft Threshold Reparameterization (STR), a novel use of the soft-threshold operator on DNN weights. STR smoothly induces sparsity while learning pruning thresholds thereby obtaining a non-uniform sparsity budget. Our method achieves state-of-the-art accuracy for unstructured sparsity in CNNs (ResNet50 and MobileNetV1 on ImageNet-1K), and, additionally, learns non-uniform budgets that empirically reduce the FLOPs by up to 50%. Notably, STR boosts the accuracy over existing results by up to 10% in the ultra sparse (99%) regime and can also be used to induce low-rank (structured sparsity) in RNNs. In short, STR is a simple mechanism which learns effective sparsity budgets that contrast with popular heuristics. Code, pretrained models and sparsity budgets are at <a href="https://github.com/RAIVNLab/STR">https://github.com/RAIVNLab/STR</a>.</i><p>

<pre xml:space="preserve">
@InProceedings{Kusupati20
  author    = {Kusupati, Aditya and Ramanujan, Vivek and
    Somani, Raghav and Wortsman, Mitchell and 
    Jain, Prateek and Kakade, Sham and Farhadi, Ali},
  title     = {Soft Threshold Weight Reparameterization 
    for Learnable Sparsity},
  booktitle = {Proceedings of the International 
    Conference on Machine Learning},
  month     = {July},
  year      = {2020},
}
</pre>
      </div>
   </td>
</tr>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="6.5">
    <tr>
     <td width="0%" valign="top" align="center"><a href="pubs/prabhu20.pdf" target="_blank"><img src="images/prabhu20.png" alt="XReg" width="100%" style="border-radius:15px"></a>
     <td width="100%" valign="top">
        <p><strongred style="color:red;">[WSDM'20]</strongred><a href="pubs/prabhu20.pdf" target="_blank" id="Prabhu20">
        <heading>Extreme Regression for Dynamic Search Advertising</heading></a><br>
        <a href="https://vervenumen.github.io" target="_blank">Yashoteja Prabhu</a>, <strong>Aditya Kusupati</strong>, Nilesh Gupta and <a href="http://manikvarma.org" target="_blank">Manik Varma</a><br>
        International Conference on Web Search and Data Mining (<b>WSDM</b>), 2020<br>
    </p>
    Long Oral presentation<br>
        <div class="paper" id="prabhu20">
        <!-- <a href="javascript:toggleblock('prabhu20abs')">abstract</a> /
        <a shape="rect" href="javascript:togglebib('prabhu20')" class="togglebib">bibtex</a> / -->
        <a href="pubs/prabhu20.pdf" target="_blank">pdf</a> /
        <a href="links/prabhu20reviews.html" target="_blank">reviews</a> /
        <a href="https://arxiv.org/abs/2001.05228" target="_blank">arXiv</a> /
        <a href="http://manikvarma.org/code/XReg/download.html" target="_blank">code</a> /
        <a href="docs/posters/prabhu20Poster.pdf" target="_blank">poster</a> /
        <a href="http://manikvarma.org/downloads/XC/XMLRepository.html" target="_blank">XML Repository</a>
        <br>
        Also <a href="pubs/prabhu20-xc.pdf" target="_blank">presented</a> at the Workshop on eXtreme Classification: Theory and Applications @ ICML, 2020<br>
        <p align="justify"> <i id="prabhu20abs">This paper introduces a new learning paradigm called eXtreme Regression (XR) whose objective is to accurately predict the numerical degrees of relevance of an extremely large number of labels to a data point. XR can provide elegant solutions to many large-scale ranking and recommendation applications including Dynamic Search Advertising (DSA). XR can learn more accurate models than the recently popular extreme classifiers which incorrectly assume strictly binary-valued label relevances. Traditional regression metrics which sum the errors over all the labels are unsuitable for XR problems since they could give extremely loose bounds for the label ranking quality. Also, the existing regression algorithms won't efficiently scale to millions of labels. This paper addresses these limitations through: (1) new evaluation metrics for XR which sum only the k largest regression errors; (2) a new algorithm called XReg which decomposes XR task into a hierarchy of much smaller regression problems thus leading to highly efficient training and prediction. This paper also introduces a (3) new labelwise prediction algorithm in XReg useful for DSA and other recommendation tasks.<br>
        Experiments on benchmark datasets demonstrated that XReg can outperform the state-of-the-art extreme classifiers as well as large-scale regressors and rankers by up to 50% reduction in the new XR error metric, and up to 2% and 2.4% improvements in terms of the propensity-scored precision metric used in extreme classification and the click-through rate metric used in DSA respectively. Deployment of XReg on DSA in Bing resulted in a relative gain of 58% in revenue and 27% in query coverage. XReg's source code can be downloaded from <a href="http://manikvarma.org/code/XReg/download.html">http://manikvarma.org/code/XReg/download.html</a>.</i></p>

<pre xml:space="preserve">
@InProceedings{Prabhu20,
  author    = {Prabhu, Prabhu and Kusupati, Aditya and 
    Gupta, Nilesh and Varma, Manik},
  title     = {Extreme Regression for Dynamic 
    Search Advertising},
  booktitle = {Proceedings of the ACM International 
    Conference on Web Search and Data Mining},
  month     = {February},
  year      = {2020},
}
</pre>
        </div>
     </td>
  </tr>


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="6.5">
    <tr>
     <td width="0%" valign="top" align="center"><a href="pubs/roy19.pdf" target="_blank"><img src="images/roy19.png" alt="MSC-RNN" width="100%" style="border-radius:15px"></a>
     <td width="100%" valign="top">
        <p><strongred style="color:red;">[BuildSys'19]</strongred><a href="pubs/roy19.pdf" target="_blank" id="Roy19">
        <heading>One Size Does Not Fit All: Multi-Scale, Cascaded RNNs for Radar Classification</heading></a><br>
        Dhrubojyoti Roy*, Sangeeta Srivastava*, <strong>Aditya Kusupati</strong>, <a href="https://pranshu93.github.io/" target="_blank">Pranshu Jain</a>, <a href="http://manikvarma.org" target="_blank">Manik Varma</a> and <a href="http://web.cse.ohio-state.edu/~arora.9/" target="_blank">Anish Arora</a><br>
        International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation (<b>BuildSys</b>), 2019<br>      
    </p>
    Oral presentation &#x1F3C6; <b>Best Paper Runner-Up Award</b><br>
        <div class="paper" id="roy19">
        <!-- <a href="javascript:toggleblock('roy19abs')">abstract</a> /
        <a shape="rect" href="javascript:togglebib('roy19')" class="togglebib">bibtex</a> / -->
        <a href="pubs/roy19.pdf" target="_blank">pdf</a> /
        <a href="docs/reviews/roy19-buildsys-reviews.txt" target="_blank">reviews</a> /
        <a href="https://arxiv.org/abs/1909.03082" target="_blank">arXiv</a> /
        <a href="https://github.com/dhruboroy29/MSCRNN" target="_blank">code</a> /
        <a href="docs/posters/roy19Poster.pdf" target="_blank">poster</a> /
        <a href="https://doi.org/10.5281/zenodo.3451408" target="_blank">dataset</a> /
        <a href="https://news.cs.washington.edu/2019/12/23/allen-schools-aditya-kusupati-earns-best-paper-runner-up-at-buildsys-2019-for-new-low-power-deep-learning-algorithm-for-radar-classification/" target="_blank">news</a>
        <br>
        Invited <a href="pubs/roy19-tosn.pdf" target="_blank">Paper</a> in ACM Transactions on Sensor Networks (<b>TOSN</b>), 2021<br>
        <p align="justify"> <i id="roy19abs">Edge sensing with micro-power pulse-Doppler radars is an emergent domain in monitoring and surveillance with several smart city applications. Existing solutions for the clutter versus multi-source radar classification task are limited in terms of either accuracy or efficiency, and in some cases, struggle with a trade-off between false alarms and recall of sources. We find that this problem can be resolved by learning the classifier across multiple time-scales. We propose a multi-scale, cascaded recurrent neural network architecture, MSC-RNN, comprised of an efficient multi-instance learning (MIL) Recurrent Neural Network (RNN) for clutter discrimination at a lower tier, and a more complex RNN classifier for source classification at the upper tier. By controlling the invocation of the upper RNN with the help of the lower tier conditionally, MSC-RNN achieves an overall accuracy of 0.972. Our approach holistically improves the accuracy and per-class recalls over machine learning models suitable for radar inferencing. Notably, we outperform cross-domain handcrafted feature engineering with purely time-domain deep feature learning, while also being up to ~3x more efficient than a competitive solution.</i></p>

<pre xml:space="preserve">
@InProceedings{Roy19,
  author    = {Roy, Dhrubojyoti and Srivastava, Sangeeta 
    and Kusupati, Aditya and Jain, Pranshu and 
    Varma, Manik and Arora, Anish},
  title     = {One Size Does Not Fit All: 
    Multi-Scale, Cascaded RNNs for 
    Radar Classification},
  booktitle = {Proceedings of the ACM International 
    Conference on Systems for Energy-Efficient 
    Buildings, Cities, and Transportation},
  month     = {November},
  year      = {2019},
}
</pre>
        </div>
     </td>
  </tr>


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="6.5">
  <tr>
     <td width="0%" valign="top" align="center"><a href="pubs/kusupati18.pdf" target="_blank"><img src="images/kusupati18.png" alt="FastGRNN" width="100%" style="border-radius:15px"></a>
     <td width="100%" valign="top">
        <p><strongred style="color:red;">[NeurIPS'18]</strongred><a href="pubs/kusupati18.pdf" target="_blank" id="Kusupati18">
        <heading>FastGRNN: A Fast, Accurate, Stable and Tiny
Kilobyte Sized Gated Recurrent Neural Network</heading></a><br>
        <strong>Aditya Kusupati</strong>, Manish Singh, <a href="https://people.eecs.berkeley.edu/~kush/" target="_blank">Kush Bhatia</a>, <a href="https://ashishkumar1993.github.io/" target="_blank">Ashish Kumar</a>, <a href="http://prateekjain.org" target="_blank">Prateek Jain</a> and <a href="http://manikvarma.org" target="_blank">Manik Varma</a><br>
        Neural Information Processing Systems (<b>NeurIPS</b>), 2018<br>
        </p>

        <div class="paper" id="kusupati18">
        <!-- <a href="javascript:toggleblock('kusupati18abs')">abstract</a> /
        <a shape="rect" href="javascript:togglebib('kusupati18')" class="togglebib">bibtex</a> / -->
        <a href="pubs/kusupati18.pdf" target="_blank">pdf</a> /
        <a href="docs/reviews/kusupati18-neurips-reviews.pdf" target="_blank">reviews</a> /
        <a href="https://arxiv.org/abs/1901.02358" target="_blank">arXiv</a> /
        <a href="https://github.com/microsoft/EdgeML" target="_blank">code</a> /
        <a href="https://youtu.be/3ZpCnOWBrio" target="_blank">video</a> /
        <a href="docs/posters/kusupati18Poster.pdf" target="_blank">poster</a> /
        <a href="https://drive.google.com/drive/folders/1nxCBEpC9qOxLP7RQs9diqTUwffck9f54?usp=sharing" target="_blank">datasets</a> /
        <a href="https://www.microsoft.com/en-us/research/blog/fast-accurate-stable-and-tiny-breathing-life-into-iot-devices-with-an-innovative-algorithmic-approach/" target="_blank">blog</a>
        <br>

        <p align="justify"> <i id="kusupati18abs">This paper develops the FastRNN and FastGRNN algorithms to address the twin RNN limitations of inaccurate training and inefficient prediction. Previous approaches have improved accuracy at the expense of prediction costs making them infeasible for resource-constrained and real-time applications. Unitary RNNs have increased accuracy somewhat by restricting the range of the state transition matrix's singular values but have also increased the model size as they require a larger number of hidden units to make up for the loss in expressive power. Gated RNNs have obtained state-of-the-art accuracies by adding extra parameters thereby resulting in even larger models. FastRNN addresses these limitations by adding a residual connection that does not constrain the range of the singular values explicitly and has only two extra scalar parameters. FastGRNN then extends the residual connection to a gate by reusing the RNN matrices to match state-of-the-art gated RNN accuracies but with a 2-4x smaller model. Enforcing FastGRNN's matrices to be low-rank, sparse and quantized resulted in accurate models that could be up to 35x smaller than leading gated and unitary RNNs. This allowed FastGRNN to accurately recognize the "Hey Cortana" wakeword with a 1 KB model and to be deployed on severely resource-constrained IoT microcontrollers too tiny to store other RNN models. FastGRNN's code is available at <a href="https://github.com/Microsoft/EdgeML/">https://github.com/Microsoft/EdgeML/</a>.</i></p>

<pre xml:space="preserve">
@InProceedings{Kusupati18,
  author    = {Kusupati, Aditya and Singh, Manish and 
    Bhatia, Kush and Kumar, Ashish and 
    Jain, Prateek and Varma, Manik},
  title     = {{FastGRNN}: A Fast, Accurate, 
    Stable and Tiny Kilobyte Sized 
    Gated Recurrent Neural Network.},
  booktitle = {Advances in 
    Neural Information Processing Systems},
  month     = {December},
  year      = {2018},
}
</pre>
        </div>
     </td>
  </tr>

  

</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <br/>
  <tr><td id="WorkshopPubs"><sectionheading>&nbsp;&nbsp;Workshop Publications</sectionheading></td></tr>
</table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="6.5">
  <tr>
   <td width="0%" valign="top" align="center"><a href="pubs/shen23.pdf" target="_blank"><img src="images/wallingford20.jpg" alt="HierNet" width="100%" style="border-radius:15px"></a>
   <td width="100%" valign="top">
      <p><a href="pubs/shen23.pdf" target="_blank" id="Shen23">
      <heading>Are “Hierarchical” Visual Representations Hierarchical?</heading></a><be>
      <a href="https://ethanlshen.github.io/">Ethan Shen</a>, Ali Farhadi, <strong>Aditya Kusupati*</strong>.<br>
      Workshop on Symmetry and Geometry in Neural Representations @ NeurIPS 2023<br>
  </p>
      <div class="paper" id="shen23">
      <a href="pubs/shen23.pdf" target="_blank">pdf</a> /
      <a href="https://arxiv.org/abs/2311.05784" target="_blank">arXiv</a> /
      <a href="https://github.com/ethanlshen/HierNet" target="_blank">code/dataset (HierNet)</a>
      <br>
      </div>
   </td>
</tr>
  
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="6.5">
  <tr>
   <td width="0%" valign="top" align="center"><a href="pubs/evtimov21.pdf" target="_blank"><img src="images/evtimov21.PNG" alt="AdvShortcuts" width="80%" height="140px" style="border-radius:10px"></a>
   <td width="100%" valign="top">
      <p><strongred style="color:red;">[AdvML@ICML'21]</strongred><a href="pubs/evtimov21.pdf" target="_blank" id="Evtimov21">
      <!-- <img src="images/new.png" alt="[NEW]" width="6%" style="border-style: none">  -->
      <heading>Disrupting Model Training with Adversarial Shortcuts</heading></a><br>
      <a href="https://ivanevtimov.eu/">Ivan Evtimov</a>, <a href="https://iancovert.com/" target="_blank">Ian Covert</a>, <strong>Aditya Kusupati</strong> and <a href="https://homes.cs.washington.edu/~yoshi/">Tadayoshi Kohno</a><br>
      Workshop on Adversarial Machine Learning @ ICML, 2021
  </p>
      <div class="paper" id="evtimov21">
      <!-- <a href="javascript:toggleblock('evtimov21abs')">abstract</a> /
      <a shape="rect" href="javascript:togglebib('evtimov21')" class="togglebib">bibtex</a> / -->
      <a href="pubs/evtimov21.pdf" target="_blank">pdf</a> /
      <a href="https://arxiv.org/abs/2106.06654" target="_blank">arXiv</a> /
      <a href="#Evtimov21" target="_blank">code</a>
      <p align="justify"> <i id="evtimov21abs">When data is publicly released for human consumption, it is unclear how to prevent its unauthorized usage for machine learning purposes. Successful model training may be preventable with carefully designed dataset modifications, and we present a proof-of-concept approach for the image classification setting. We propose methods based on the notion of adversarial shortcuts, which encourage models to rely on non-robust signals rather than semantic features, and our experiments demonstrate that these measures successfully prevent deep learning models from achieving high accuracy on real, unmodified data examples.</i><p>

<pre xml:space="preserve">
@article{Evtimov21
  author    = {Evtimov, Ivan and Covert, Ian
    and Kusupati, Aditya and Kohno, Tadayoshi},
  title     = {Disrupting Model Training 
    with Adversarial Shortcuts},
  booktitle = {arXiv preprint arXiv:2106.06654},
  year      = {2021},
}
</pre>
      </div>
   </td>
</tr>
</table>

<!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <br/>
  <tr><td id="TechReports"><sectionheading>&nbsp;&nbsp;Technical Reports</sectionheading></td></tr>
</table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr>
     <td width="0%" valign="top"><a href="pubs/kusupati20a.pdf" target="_blank"><img src="images/kusupati20a.png" alt="Conclusions of the paper" height="100px" style="border-radius:15px"></a>
     <td width="100%" valign="top">
        <p><a href="pubs/kusupati20a.pdf" target="_blank" id="Kusupati20a">
          <heading>Adapting Unstructured Sparsity Techniques for Structured Sparsity</heading></a><br>
          <strong>Aditya Kusupati</strong><br>
          Technical Report, 2020<br>        
        </p>

        <div class="paper" id="kusupati20a">
        <a href="javascript:toggleblock('kusupati20aabs')">abstract</a> /
        <a shape="rect" href="javascript:togglebib('kusupati20a')" class="togglebib">bibtex</a> /
        <a href="pubs/kusupati20a.pdf" target="_blank">pdf</a> / 
        <a href="https://github.com/RAIVNLab/STR-BN/" target="_blank">code</a>

        <p align="justify"> <i id="kusupati20aabs">Unstructured and structured sparsities provide unique advantages in resource-efficient sparse neural networks. Unstructured sparsity can assist in obtaining highly sparse and accurate models, while structured sparsity focuses mainly on enabling fast parallelizable inference on commodity hardware (e.g. GPUs). In the recent past, these distinctive advantages led to the divergence of the sub-fields leading to a disconnect. In this report, we propose and argue that most recent advances in unstructured sparsity can be adapted for inducing structured sparsity in deep neural networks. We also note the similarities between both these two sub-fields and document how the solutions from unstructured sparsity can be leveraged in solving the issues of structured sparsity. We also showcase the ease of adaptation by proposing STR-BN which is an application of the recently proposed STR method on batch normalization to induce structured sparsity via filter/neuron pruning. Code for STR-BN can be found at <a href="https://github.com/RAIVNLab/STR-BN">https://github.com/RAIVNLab/STR-BN</a>.</i></p>

<pre xml:space="preserve">
@article{Kusupati20a,
  author    = {Kusupati, Aditya},
  title     = {Adapting Unstructured Sparsity 
    Techniques for Structured Sparsity},
  booktitle = {Technical Report},
  month     = {August},
  year      = {2020},
}</pre>
        </div>
     </td>
  </tr>
</table> -->

<!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <br/>
  <tr><td id="Demos"><sectionheading>&nbsp;&nbsp;Demos</sectionheading></td></tr>
</table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr>
     <td width="0%" valign="top"><a href="pubs/roy19-demo.pdf" target="_blank"><img src="images/roy19-demo.jpg" alt="Radar for MSC-RNN demo" width="100%" style="border-radius:15px"></a>
     <td width="100%" valign="top">
        <p><a href="pubs/roy19-demo.pdf" target="_blank" id="Roy19-demo">
          <heading>Lightweight, Deep RNNs for Radar Classification</heading></a><br>
          Dhrubojyoti Roy*, Sangeeta Srivastava*, <a href="https://pranshu93.github.io/" target="_blank">Pranshu Jain</a>, <strong>Aditya Kusupati</strong>, <a href="http://manikvarma.org" target="_blank">Manik Varma</a> and <a href="http://web.cse.ohio-state.edu/~arora.9/" target="_blank">Anish Arora</a><br>
          International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation (<b>BuildSys</b>), 2019<br>        
        </p>

        <div class="paper" id="roy19-demo">
        <a href="javascript:toggleblock('roy19-demoabs')">abstract</a> /
        <a shape="rect" href="javascript:togglebib('roy19-demo')" class="togglebib">bibtex</a> /
        <a href="pubs/roy19-demo.pdf" target="_blank">pdf</a>

        <p align="justify"> <i id="roy19-demoabs">We demonstrate Multi-Scale, Cascaded RNN (MSC-RNN), an energy-efficient recurrent neural network for real-time micro-power radar classification. Its two-tier architecture is jointly trained to reject clutter and discriminate displacing sources at different time-scales, with a lighter lower tier running continuously and a heavier upper tier invoked infrequently on an on-demand basis. It offers for single microcontroller devices a better trade-off in accuracy and efficiency, as well as in clutter suppression and detectability, over competitive shallow and deep alternatives.</i></p>

<pre xml:space="preserve">
@InProceedings{Roy19-demo,
  author    = {Roy, Dhrubojyoti and Srivastava, Sangeeta 
    and Jain, Pranshu and Kusupati, Aditya and 
    Varma, Manik and Arora, Anish},
  title     = {Lightweight, Deep RNNs 
    for Radar Classification},
  booktitle = {Proceedings of the ACM International 
    Conference on Systems for Energy-Efficient 
    Buildings, Cities, and Transportation},
  month     = {November},
  year      = {2019},
}</pre>
        </div>
     </td>
  </tr>
</table> -->


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <br/>
  <tr><td id="Theses"><sectionheading>&nbsp;&nbsp;Theses</sectionheading></td></tr>
</table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="6.5">
  <tr>
     <td width="0%" valign="top"><a href="pubs/kusupati24.pdf" target="_blank"><img src="images/kusupati17.png" alt="Towards Adaptive Intelligence" width="100%" style="border-radius:15px"></a>
     <td width="100%" valign="top">
        <p><strongred style="color:red;">[PhD Thesis'24]</strongred><a href="pubs/kusupati24.pdf" target="_blank" id="Kusupati24">
        <heading>Towards Adaptive Intelligence</heading></a><br>
        <strong>Aditya Kusupati</strong><br>
        PhD Thesis, Paul G. Allen School of Computer Science & Engineering, University of Washington, 2019-24
        <br></p>

        <div class="paper" id="kusupati24">
        <a href="pubs/kusupati24.pdf" target="_blank">pdf</a>
        </div>
     </td>
  </tr>
</table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="6.5">
  <tr>
     <td width="0%" valign="top"><a href="pubs/kusupati17.pdf" target="_blank"><img src="images/kusupati17.png" alt="geometric embeddings" width="100%" style="border-radius:15px"></a>
     <td width="100%" valign="top">
        <p><strongred style="color:red;">[UG Thesis'17]</strongred><a href="pubs/kusupati17.pdf" target="_blank" id="Kusupati17">
        <heading>Efficient Spatial Representation for Entity-Typing</heading></a><br>
        Anand Dhoot*, <strong>Aditya Kusupati*</strong> and <a href="https://www.cse.iitb.ac.in/~soumen/" target="_blank">Soumen Chakrabarti</a><br>
        Undergraduate Thesis, Department of Computer Science & Engineering, IIT Bombay, 2016-17
        <br></p>

        <div class="paper" id="kusupati17">
        <!-- <a href="javascript:toggleblock('kusupati17abs')">abstract</a> /
        <a shape="rect" href="javascript:togglebib('kusupati17')" class="togglebib">bibtex</a> / -->
        <a href="pubs/kusupati17.pdf" target="_blank">pdf</a>

        <p align="justify"> <i id="kusupati17abs">The project aims at creating a efficient spatial embeddings for entities and types which would be useful for various downstream tasks such as Knowledge Base Completion, Fine-Type Tagging and Question Answering.</i></p>

<pre xml:space="preserve">
@article{Kusupati17,
  author = {Dhoot, Anand and Kusupati, Aditya 
    and Chakrabarti, Soumen},
  title = {Efficient Spatial Representation 
    for Entity-Typing},
  booktitle = {Undergraduate Thesis, CSE IIT Bombay},
  year = {2016-17},
}</pre>
        </div>
     </td>
  </tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <br/>
  <tr><td id="Software"><sectionheading>&nbsp;&nbsp;Software</sectionheading></td></tr>
</table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="6.5">
  <tr>
    <td width="0%" valign="top"><a href="https://github.com/microsoft/EdgeML" target="_blank"><img src="images/edgeml.png" alt="EdgeML" width="100%" style="border-radius:15px"></a>
     <td width="100%" valign="top">
        <p><a href="https://github.com/microsoft/EdgeML" id="EdgeML" target="_blank">
        <heading>EdgeML: Machine Learning for resource-constrained edge devices</heading></a><br>
        Work of many amazing collaborators. I was one of the initial and primary contributors.<br> 
        Github, Microsoft Research India, 2017-present.
        <br></p>

        <div class="paper" id="edgeml">
        <a href="javascript:toggleblock('edgemlabs')">abstract</a> /
        <a shape="rect" href="javascript:togglebib('edgeml')" class="togglebib">bibtex</a>

        <!-- <p align="justify"> <i id="edgemlabs">Open source repository for all the research outputs on resource efficient Machine Learning from Microsoft Research India. It contains scalable and multi-framework compatible implementations of Bonsai, ProtoNN, FastCells, EMI-RNN, ShaRNN, RNNPool, DROCC, a tool named SeeDot for fixed-point compilation of ML models along with applications such as on-device Keyword spotting and Gesturepod.</i><br>EdgeML is under MIT license and is open to contributions and suggestions. Please cite the software if you happen to use EdgeML in your research or otherwise (use the latest bibtex from the <a href="https://github.com/microsoft/EdgeML" target="_blank">repository</a>).</p> -->

<pre xml:space="preserve">
@misc{edgeml03,
    author = {{Dennis, Don Kurian and Gaurkar, Yash and 
      Gopinath, Sridhar and Goyal, Sachin 
      and Gupta, Chirag and Jain, Moksh 
      and Kumar, Ashish and Kusupati, Aditya 
      and Lovett, Chris and Patil, Shishir Girish 
      and Oindrila Saha and Simhadri, Harsha Vardhan}},
    title = {{EdgeML: Machine Learning 
      for resource-constrained edge devices}},
    url = {https://github.com/Microsoft/EdgeML},
    version = {0.3},
}</pre>
        </div>
     </td>
  </tr>
</table>

<!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td id="Teaching"><sectionheading>&nbsp;&nbsp;Teaching</sectionheading></td></tr>
</table>
<table width="100%" align="top" border="0" cellpadding="20">
  <tr>
    <td width="0%" valign="top"><a href="#Teaching"><img src="images/teaching.png" alt="teaching" width="90%" style="border-radius:15px"></a>
     <td width="100%" valign="top">
        <p>
          <a href="https://www.cse.iitb.ac.in/~supratik/courses/cs226/spr17/" target="_blank"><b>CS226/254: Digital Logic Design + Lab - Spring '17, IIT Bombay</b></a><br>
        </p>
        <p>
          <a href="#Teaching"><b>CS251: Software Systems Lab - Fall '16, IIT Bombay</b></a><br>
        </p>
        <p>
          <a href="https://www.cse.iitb.ac.in/~supratik/courses/cs226/spr16/" target="_blank"><b>CS226/254: Digital Logic Design + Lab - Spring '16, IIT Bombay</b></a><br>
        </p>
        <p>
          <a href="#Teaching"><b>CS101: Computer Programming and Utilisation - Fall '15, IIT Bombay</b></a><br>
        </p>
        <p>
          <a href="https://www.cse.iitb.ac.in/~cs101/2015.1/" target="_blank"><b>CS101: Computer Programming and Utilisation - Spring '15, IIT Bombay</b></a><br>
        </p>
     </td>
  </tr>
</table> -->

<table width="100%" align="center" border="0" cellpadding="10">
  <tr><td id="TeachingServiceTalks">
      <sectionheading>&nbsp;&nbsp;Teaching, Service & Talks</sectionheading><br>
      <ul>
      Check my <a href="docs/AdityaKusupati_Research_CV.pdf" target="_blank">CV</a> for these details.
    </ul>
    </td>
  </tr>
</table>
<!-- 
<table width="100%" align="center" border="0" cellpadding="10">
  <tr><td id="Service">
      <sectionheading>&nbsp;&nbsp;Service</sectionheading>
      <ul>
      <li>Workshops</li>
      <ul>
        <li><a href="https://rethinkingmlpapers.github.io/" target="_blank">Rethinking ML Papers</a> workshop @ ICLR '21</li>
      </ul>
      <li>Reviewing</li>
        <ul>
          <li>IEEE TPAMI</li>
          <li>NeurIPS <a href="https://neurips.cc/Conferences/2019/Reviewers" target="_blank">'19</a>, <a href="https://neurips.cc/Conferences/2020/ProgramCommittee" target="_blank">'20</a>, <a href="https://neurips.cc/Conferences/2021/ProgramCommittee" target="_blank">'21</a></li>
          <li>ICML <a href="https://icml.cc/Conferences/2020/Reviewers" target="_blank">'20</a>, <a href="https://icml.cc/Conferences/2021/Reviewers" target="_blank">'21</a></li>
          <li>ICLR <a href="https://iclr.cc/Conferences/2021/Reviewers" target="_blank">'21</a>, '22</li>
          <li>CVPR <a href="http://cvpr2021.thecvf.com/node/184" target="_blank"></a>'21</li>
          <li>ICCV <a href="http://iccv2021.thecvf.com/emergency-reviewers" target="_blank">'21</a></li>
        </ul>
      <li>Mentorship</li>
      <ul>
        <li><a href="http://newinml.org" target="_blank">New In ML 2019 session</a> @ NeurIPS '19</li>
        <li><a href="https://www.microsoft.com/en-us/research/event/microsoft-research-summer-workshop-2018-machine-learning-on-constrained-devices/" target="_blank">MSR India workshop for machine learning on constrained devices</a>, Summer 2018</li>
      </ul>
      <li>Co-Lead, Allen School PhD Pre-Application Review Service (PARS) (2020)</li>
      <li>Faculty Recruiting Liason, CSE, University of Washington (2020-21)</li>
      <li>Department General Secretary, CSE, IIT Bombay (2016-17)</li>
    </ul>
  </td></tr>
</table>

<table width="100%" align="center" border="0" cellpadding="10">
  <tr><td id="Talks">
     <sectionheading>&nbsp;&nbsp;Talks</sectionheading>
     <ul>
     <li>Soft Threshold Weight Reparameterization for Learnable Sparsity</li>
     <ul>
     <li> International Conference on Machine Learning (ICML) (July '20)</li>
     <li> NVIDIA Research (July '20)</li>
     <li> Deep Learning: Classics and Trends (June '20)</li>
     </ul>
     <li>The Edge of Machine Learning</li>
     <ul>
     <li> University of Washington Sensor Systems Seminar (October '19)</li> 
     <li> University of Washington CSE Colloquium (October '19)</li>
     <li> VGG @ Oxford University, UK on (April '19)</li>
     <li> Microsoft Research Redmond (March '19)</li>
     <li> Microsoft Research India (August '18)</li>
     </ul>
     <li>The Extremes of Machine Learning</li>
     <ul>
     <li> Microsoft Bing Bellevue (March '19)</li>
    </ul>
     </ul>
  </td></tr>
</table> -->


<table width="100%" align="center" border="0" cellpadding="10">
  <tr><td id="Misc">
     <sectionheading>&nbsp;&nbsp;Misc</sectionheading>
     <ul>
      <li> Some <a href="javascript:toggleblock('gradschool')">amazing resources</a> on CS grad schools apps (especially for ML).</li>
      <p align="justify" id="gradschool">
          <a href="https://h2r.cs.brown.edu/writing-a-research-statement-for-graduate-school-and-fellowships/" target="_blank">Writing a Research Statement for Graduate School and Fellowships</a><br>Stefanie Tellex<br><br>
          <a href="https://timdettmers.com/2018/11/26/phd-applications/" target="_blank">Machine Learning PhD Applications — Everything You Need to Know</a><br>Tim Dettmers<br><br>
          <a href="https://timdettmers.com/2020/03/10/how-to-pick-your-grad-school/" target="_blank">How to Pick Your Grad School</a><br>Tim Dettmers<br><br>
          <a href="http://www.cs.cmu.edu/~harchol/gradschooltalk.pdf" target="_blank">Applying to Ph.D. Programs in Computer Science</a><br>Mor Harchol-Balter<br><br>
          <a href="https://da-data.blogspot.com/2015/03/reflecting-on-cs-graduate-admissions.html" target="_blank">Reflecting on CS Graduate Admissions</a><br>Dave Andersen<br><br>
          <a href="https://blog.nelsonliu.me/2019/10/24/student-perspectives-on-applying-to-nlp-phd-programs/" target="_blank">Student Perspectives on Applying to NLP PhD Programs</a><br>Akari Asai, John Hewitt, Sidd Karamcheti, Kalpesh Krishna, Nelson Liu, Roma Patel, and Nicholas Tomlin.<br><br>
          <a href="http://martiansideofthemoon.github.io/2018/05/29/grad-resources.html" target="_blank">One stop destination for many great resources</a> (I gave this name. The collection is impressive.)<br>Kalpesh Krishna
          <br><br>
          <a href="https://parentheticallyspeaking.org/articles/us-cs-phd-faq/" target="_blank">Getting a Computer Science PhD in the USA</a><br> Shriram Krishnamurthi
      </p>
     <li> A <a href="docs/Misc/How-to-become-Batman.pdf" target="_blank">presentation</a> on <b>"How to become Batman?"</b></li>
     <li> Some <a href="javascript:toggleblock('quotes')">quotes</a>  I came up/co-came up with.</li>
     <p align="justify" id="quotes">
        <em>"Time to Se(a)ttle. Settle in Seattle." - Aditya Kusupati</em><br><br>
        <em>"When everything is same kind of unique, nothing is unique." - Aditya Kusupati</em><br><br>
        <em>"</em>&#128578; <em>is the scariest emoticon out there." - Aditya Kusupati</em><br><br>
        <em>"Humans are destructive by construction." - Aditya Kusupati</em><br><br>
        <em>"The world is unduly biased towards extroverts." - Suma Kasa and Aditya Kusupati</em><br><br>
        <em>"Illusion of choice paralyzes a person" - Aditya Kusupati</em>
     </p>
     <li> Some <a href="javascript:toggleblock('quotes1')">quotes</a>  I really like.</li>
     <p align="justify" id="quotes1">
      <em>"When you say you're fucked, you are only 45% fucked." - Nims Purja</em>
   </p>
     <li> These <a href="https://www.cs.hmc.edu/~fleck/parable.html" target="_blank">two parables</a> crack me up every single time. I am waiting to find out my side.</li>
     <!-- <li> Few more <a href="javascript:toggleblock('acknowledgements')">papers</a> where my insights were found helpful and were acknowledged</li>
     <p align="justify" id="acknowledgements">
        <a href="https://arxiv.org/abs/2002.12718" target="_blank">DROCC: Deep Robust One-Class Classification</a><br>Sachin Goyal, Aditi Raghunathan, Moksh Jain, Harsha Vardhan Simhadri and Prateek Jain<br><i>ICML '20</i><br><br>
        <a href="https://arxiv.org/abs/1906.02256" target="_blank">Butterfly Transform: An Efficient FFT Based Neural Architecture Design</a><br>Keivan Alizadeh-Vahid, Anish Prabhu, Ali Farhadi and Mohammad Rastegari<br><i>CVPR '20</i><br><br>
        <a href="http://manikvarma.org/pubs/patil19.pdf" target="_blank">GesturePod: Enabling On-device Gesture-based Interaction for White Cane Users</a><br>Shishir G. Patil, Don Kurian Dennis, Chirag Pabbaraju, Nadeem Shaheer, Harsha Vardhan Simhadri, Vivek Seshadri, Manik Varma and Prateek Jain<br><i>UIST '19</i><br><br>
        <a href="https://doi.org/10.1145/3314221.3314597" target="_blank">Compiling KB-Sized Machine Learning Models to Tiny IoT Devices</a><br>Sridhar Gopinath, Nikhil Ghanathe, Vivek Seshadri, Rahul Sharma<br><i>PLDI '19</i><br><br>
        <a href="https://arxiv.org/pdf/1805.04690.pdf" target="_blank">New Embedded Representations and Evaluation Protocols for Inferring Transitive Relations</a><br>Sandeep Subramanian and Soumen Chakrabarti<br><i>SIGIR '18</i>   
     </p> -->
     <!-- <li> Manish Singh and I discovered <a href="https://arxiv.org/abs/1710.05941" target="_blank">Swish</a> in the same timeline as the paper</li> -->
     <li> Interned at American Express Big Data Labs with <a href="https://research.adobe.com/person/vishwa-vinay/" target="_blank">Vishwa Vinay</a> and Madhan RA (Summer '16)</li>
     <li> Interned with <a href="https://team.inria.fr/titane/pierre-alliez/" target="_blank">Pierre Alliez</a> at <a href="https://www.inria.fr/en/centre/sophia" target="_blank">Inria Sophia Antipolis Mediterranean</a> located in the breathtakingly beautiful French Riviera (Summer '15)</li>
     <li> Used to play Lawn Tennis till about 2014 and won state-level championships in Andhra Pradesh, India</li>
     <li> If you like my CV template, you can find it at <a href="https://github.com/adityakusupati/Research_CV_Template">adityakusupati/Research_CV_Template</a></li>
     <li> My brother does some amazing work in Computer Vision and Geometry Processing. Check out his <a href="http://udaykusupati.github.io/" target="_blank">research</a>.
     </ul>
  </td></tr>
</table>

<font size="3">This Flag counter seemed fun. It shows counts since March 24, 2020.</font>
<a href="https://info.flagcounter.com/qdQA"><img src="https://s05.flagcounter.com/count2/qdQA/bg_FFFFFF/txt_000000/border_CCCCCC/columns_6/maxflags_150/viewers_0/labels_0/pageviews_1/flags_0/percent_0/" alt="Flag Counter" border="0"></a>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
     <tr><td><br><p align="right"><font size="2">
     Template: <a href="https://jonbarron.info">this</a>, <a href="https://people.eecs.berkeley.edu/~pathak/">this</a> and <a href="http://saurabhg.web.illinois.edu/">this</a>
     </font></p></td></tr>
</table>

  </td></tr>
</table>
<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('rege23abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('wallingford23abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('kusupati22abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('zellers22abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('jain22abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('evtimov21abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('kusupati21abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('wallingford20abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('saha20abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('kusupati20abs');
</script>
<script xml:space="preserve" language="JavaScript">
    hideblock('prabhu20abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('kusupati18abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('roy19abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('roy19-demoabs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('kusupati20aabs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('kusupati17abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('edgemlabs');
</script>
<!-- <script xml:space="preserve" language="JavaScript">
hideblock('acknowledgements');
</script> -->
<script xml:space="preserve" language="JavaScript">
  hideblock('gradschool');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('quotes');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('quotes1');
</script>

</body>

</html>
